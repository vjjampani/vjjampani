---
title: "DSC520 Week8,9 Housing Data Exercise 8.2"
author: "Venkat Jagadeesh Jampani"
date: February 12th 2022
output:
  word_document: default
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=TRUE}
library(readxl)
library(dplyr)
library(purrr)
library(ggplot2)
library(lmtest)
library(lm.beta)
library(car)
```

```{r echo=TRUE}
## Set workding directory to read source datasets.
setwd("/Users/Jagadeesh/Documents/GitHub/dsc520")
## Read housing dataset
housingdata_df <- read_excel("data/week-6-housing.xlsx")
head(housingdata_df)

str(housingdata_df)
glimpse(housingdata_df)
## Check for nulls in all rows
apply(housingdata_df, 2, function(i) any(is.na(i)))
## Looking at the data, there is missing data for sale_warning and ctyname
```

```{r echo=TRUE}
# I. Explain any transformations or modifications you made to the dataset 
colnames(housingdata_df)[1] <- "Sale_Date"
colnames(housingdata_df)[2] <- "Sale_Price"
# I have Changed the column names of "Sale Date" to "Sale_Date" and "Sale Price" to "Sale_Price" to avoid issues.

# II. Create two variables;
#    one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression)
#    and one that will contain Sale Price and several additional predictors of your choice.
#    Explain the basis for your additional predictor selections.
housingdata_lm1 <- lm(formula = Sale_Price ~ sq_ft_lot, data = housingdata_df)
housingdata_lm1
housingdata_lm2 <- lm(formula = Sale_Price ~ zip5 + bedrooms + bath_full_count + year_built, data = housingdata_df)
housingdata_lm2

# I have included other predictors like zip5, bedrooms, bath_full_count and year of built as those are important key factors in home price predictions.
# III. Execute a summary() function on two variables defined in the previous step to compare the model results. 
#    What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model.
#    Did the inclusion of the additional predictors help explain any large variations found in Sale Price?
summary(housingdata_lm1)
summary(housingdata_lm2)


# R2 for housingdata_lm1: 0.01435  adjusted: 0.01428
# R2 for housingdata_lm2: 0.11263 adjusted: 0.1126
# R-Squared is a statistical measure of fit for the model.
# These low R-Squared values mean that the model is not a good fit for analysis.
# The multiple regression seems better, but not ideal for the given data set.
# IV. Considering the parameters of the multiple regression model you have created,
#     What are the standardized betas for each parameter and what do the values indicate?
coef_lmbeta <- lm.beta(housingdata_lm2)
coef_lmbeta

# zip5 (standardized β = 0.03485771) - This value indicates that as zip code increase by 1 standard deviation, 
# the sales price increase by 0.03485771 standard deviation.
# bedrooms (standardized β = 0.17877693) -This value indicates that as bedrooms increase by 1 standard deviation, 
# the sales price of the house increase by 0.17877693 standard deviation.
# bath_full_count (standardized β =  0.15058250) -This value indicates that as full bath room  increase by 1 standard deviation, 
# the sales price of the house increase by  0.15058250 standard deviation.
# year_built(standardized β = 0.16877309 ) - This value indicates that as year_# built increase by 1 standard deviation, 
# the sales price increase by 0.16877309 standard deviation.

# V. Calculate the confidence intervals for the parameters in your model and
#     explain what the results indicate.
confint(housingdata_lm2)

# In the selected model, the predictor (year_built) have very tight confidence intervals,
# which describes that the estimates for the current model are likely
# to be representative of the true population.
# The confidence interval for (zip5, bedrooms, bath_full_count) is wider but still does not cross zero,
# indicating that the parameter for this variable is less representative of the population, 
# but are still significant for the selected model.

# VI. Assess the improvement of the new model compared to your original model (simple regression model) ----
#     by testing whether this change is significant by performing an analysis of variance.
anova(housingdata_lm1,housingdata_lm2)
## The p value is very small value indeed,
## we can say that housingdata_lm2 significantly improved
## the fit of the model to the data compared to housingdata_lm1
# VII. Perform casewise diagnostics to identify outliers and/or influential cases,
#     storing each function's output in a dataframe assigned to a unique variable name.
housingdata_df$residuals<-resid(housingdata_lm2)
housingdata_df$standardized.residuals<- rstandard(housingdata_lm2)
housingdata_df$studentized.residuals<-rstudent(housingdata_lm2)
housingdata_df$cooks.distance<-cooks.distance(housingdata_lm2)
housingdata_df$dfbeta<-dfbeta(housingdata_lm2)
housingdata_df$dffit<-dffits(housingdata_lm2)
housingdata_df$leverage<-hatvalues(housingdata_lm2)
housingdata_df$covariance.ratios<-covratio(housingdata_lm2)

head(housingdata_df)

# VIII. Calculate the standardized residuals using the appropriate command,
#     specifying those that are +-2, storing the results of large residuals in a variable you create.
housingdata_df$large.residual <-housingdata_df$standardized.residuals >2 | housingdata_df$standardized.residuals < -2
head(housingdata_df$large.residual)


# IX. Use the appropriate function to show the sum of large residuals.
sum(housingdata_df$large.residual)
# X. Which specific variables have large residuals (only cases that evaluate as TRUE)?
housingdata_df[housingdata_df$large.residual,c("Sale_Price", "zip5", "bedrooms", "bath_full_count", "year_built", "standardized.residuals")]
# XI. Investigate further by calculating the
#    leverage,
#    cooks distance,
#    and covariance ratios.
# Comment on all cases that are problematics.
housingdata_df[housingdata_df$large.residual , c("cooks.distance", "leverage", "covariance.ratios")]
# Except one (1.26)remaining of the values has a Cook’s distance less than 1 ,
# The leverage values is very low.
# XII. Perform the necessary calculations to assess the assumption of independence
#    and state if the condition is met or not.
durbinWatsonTest(housingdata_lm2)
## The test statistic is 0.7442029 and the corresponding p-value is 0. 
## Since this p-value is less than 0.05, we can reject the null hypothesis and 
## conclude that the residuals in this regression model are autocorrelated. 
## Value less than 1 suggests that the assumption might not been met.
# XIII. Perform the necessary calculations to assess the assumption of no multicollinearity
#    and state if the condition is met or not.
vif(housingdata_lm2)
## tolerance statistics
1/vif(housingdata_lm2)
mean(vif(housingdata_lm2))
## VIF values are all below 10 and the tolerance statistics above 0.2. 
## Also, the mean VIF is ~ 1.
## Based on these results we can conclude that there is no collinearity in data.
# XIV. Visually check the assumptions related to the residuals using the plot() and hist() functions.
#     Summarize what each graph is informing you of and if any anomalies are present.
housingdata_df$fitted <- housingdata_lm2$fitted.values
histogram<-ggplot(housingdata_df, aes(studentized.residuals)) + 
  geom_histogram(aes(y = ..density..), colour = "blue", fill = "white") + 
  labs(x = "StudentizedResidual", y = "Density")
histogram
histogram + stat_function(fun = dnorm, args = list(mean = mean(housingdata_df$studentized.residuals, na.rm = TRUE), 
  sd = sd(housingdata_df$studentized.residuals,na.rm = TRUE)), colour= "red", size = 1)
qplot(sample = housingdata_df$studentized.residuals, stat="qq") + labs(x ="Theoretical Values", y = "Observed Values")
# The distribution is briefly normal.
# To summarize, the model appears to be accurate for the sample and can be generalized to the population.
# XV. Overall, is this regression model unbiased?
#    If an unbiased regression model, what does this tell us about the sample vs. the entire population model?
# Based on vif score/values calculated above, since the values are not close to 5, the predictors doesn't have 
# any significant multi-collinearity.
# Mean vif is also just above 1 but no where near 5.
# so, Model does not appear to be biased.
```